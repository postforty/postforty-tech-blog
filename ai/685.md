# Mac Mini M4(Apple Silicon) 환경에서 Ollama 설치 및 관리 가이드

## 1. 서론
Mac Mini M4(Apple Silicon) 환경에서 Homebrew 패키지 관리자를 사용하여 로컬 LLM 추론 도구인 Ollama를 설치하고, 실행 및 관리하는 방법에 대해 안내한다. 설치 및 삭제 절차, 서비스 관리, 추가적인 주의 사항을 상세히 설명한다.
특히, 이 환경에서는 Ollama가 Apple Silicon 칩의 CPU와 GPU(Apple Metal API 기반)를 모두 자동으로 활용한다. 딥러닝 추론 시 모델의 연산은 macOS의 Metal API를 통해 GPU로 오프로드되며, 일부 보조 연산과 관리 기능은 CPU에서 동작한다. 별도의 사용자 설정 없이 Ollama의 네이티브 설치만으로 Apple Silicon 칩의 리소스를 최적화하여 사용할 수 있다는 것이 장점이다. 실제로 대형 모델을 구동하면 Activity Monitor(활동 모니터) 등에서 GPU 및 CPU 리소스가 함께 활용되는 것을 확인할 수 있다.

## 2. 설치 및 기본 환경 설정

### 2-1. Homebrew 환경 점검
+ 우선 Homebrew 자체와 포뮬러 정의를 갱신하여 최신 패키지를 이용할 수 있도록 환경을 점검해야 한다.
  ```bash
  brew update
  ```
+ 목적: Homebrew와 패키지 포뮬러를 최신화하여 Ollama 설치 시 최신 버전을 보장한다.

### 2-2. Ollama 설치
+ Ollama는 Homebrew의 공식 Core Formula를 통해 설치한다.
  ```bash
  brew install ollama
  ```
+ 분석: 위 명령은 M4(arm64) 아키텍처에 맞는 바이너리를 자동 설치하고, 실행 권한을 부여한다.

### 2-3. 실행 방식 선택
설치 후 실행 방식은 백그라운드 서비스와 포그라운드 실행 두 가지로 나뉜다.

#### 2-3-1. 백그라운드 서비스 등록(권장)
+ 시스템 부팅 시 자동으로 시작되며, 서비스(데몬)로 상주한다.
  ```bash
  brew services start ollama
  ```

#### 2-3-2. 일회성 실행(수동)
+ 현재 터미널 세션에서만 구동되며, 터미널을 닫으면 자동 종료된다.
  ```bash
  ollama serve
  ```

## 3. 설치 검증 및 기본 사용법

### 3-1. 설치 검증
+ 설치된 Ollama의 버전을 확인하여 정상적으로 설치되었는지 검증한다.
  ```bash
  ollama --version
  ```

### 3-2. 모델 다운로드 및 실행(예시: Llama 3.2)
+ 모델 파일이 없다면 첫 실행 시 자동 다운로드가 진행된다.
  ```bash
  ollama run llama3.2
  ```
+ 작동 원리: 최초 실행 시 모델 파일이 로컬에 없으면 자동으로 레지스트리에서 다운로드한다.
+ M4 최적화: Ollama는 별도 설정 없이 Metal 가속 및 unified memory를 활용하여 실행된다.

## 4. 트러블슈팅 및 팁

### 4-1. 네트워크 바인딩 변경(외부 접속)
+ Ollama는 기본적으로 `127.0.0.1:11434`에 바인딩된다. 만약 Mac Mini를 외부에서 사용할 계획이 있다면 환경 변수 설정이 필요하다.

### 4-2. 메모리 관리 및 리소스 모니터링
+ M4의 unified memory 크기에 따라 구동할 수 있는 모델의 크기가 제한된다. 메모리 스왑 발생 시 성능이 저하될 수 있으므로 리소스 점유율을 확인한다.
  ```bash
  ollama ps
  ```

#### 4-2-1. 추가 제안
+ Ollama를 외부 네트워크에서 접속하도록 "OLLAMA_HOST=0.0.0.0" 환경 변수를 설정하는 방법도 고려할 수 있다.

## 5. Ollama 서비스 및 데이터 관리

### 5-1. 백그라운드 서비스 중지 및 확인
+ 시스템 서비스로 등록된 Ollama를 중지하려면 다음 명령을 사용한다.
  ```bash
  brew services stop ollama
  ```
+ 서비스 중지 여부는 다음 명령으로 확인한다.
  ```bash
  brew services list
  ```
+ 포그라운드에서 직접 구동된 경우(tml에서 `ollama serve` 실행 중) 해당 터미널에서 `Ctrl + C`로 종료한다.

### 5-2. 저장된 모델 데이터 관리

#### 5-2-1. 모델 목록 확인
+ 저장된 모델 목록 및 태그를 확인한다.
  ```bash
  ollama list
  ```

#### 5-2-2. 특정 모델 삭제
+ 모델명을 명시하여 삭제한다. (`llama3.2:latest` 예시)
  ```bash
  ollama rm llama3.2:latest
  ```
+ latest 태그는 생략할 수 있으나, 버전을 명확하게 지정하는 것이 안전하다.

### 5-3. 완전 삭제(Uninstall) 및 데이터 삭제

#### 5-3-1. 패키지 삭제
+ Homebrew를 통해 Ollama 바이너리를 삭제한다.
  ```bash
  brew uninstall ollama
  ```

#### 5-3-2. 남아있는 데이터와 모델 파일 완전 제거
+ Ollama는 기본적으로 `~/.ollama` 폴더에 모델과 설정값을 저장하므로, 수동으로 삭제해야 용량을 완전히 확보할 수 있다.
  ```bash
  rm -rf ~/.ollama
  ```
+ 경고: `rm -rf` 명령은 복구 불가능하므로 삭제 대상이 맞는지 반드시 확인 후 실행해야 한다.

## 6. 결론 및 후속 제안

단순히 재설치를 원하는 경우, 모델 데이터까지 삭제할 필요는 없다. 바이너리만 제거 후 재설치하면 기존 모델은 그대로 활용 가능하다. 필요시 LM Studio, GPT4All 등 다른 로컬 LLM 실행 도구와의 비교도 고려할 수 있다.

---